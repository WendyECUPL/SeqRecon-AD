{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb3bb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                card_id         settle_time       明细项目名称\n",
      "0  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48        尼可地尔片\n",
      "1  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48  头孢克洛缓释片(II)\n",
      "2  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48      乳果糖口服溶液\n",
      "3  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48        艾司唑仑片\n",
      "4  dccc6fc4-d367-420f-846d-ef5ece5cc1d2 2023-03-13 09:43:49    盐酸地尔硫卓缓释片\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1107985"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 导入依赖 & 读取数据\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 载入明细和标签，字段名与 featureprocessing-Copy1.ipynb 保持一致\n",
    "df = pd.read_csv('model1_data.csv', encoding='gbk', low_memory=False)\n",
    "dflabel = pd.read_csv('model1_label.csv', encoding='gbk')\n",
    "\n",
    "# 重命名字段并合并标签\n",
    "df.rename(columns={\n",
    "    '卡号': 'card_id',\n",
    "    '机构名称': 'org_id',\n",
    "    '结算日期时间': 'settle_time',\n",
    "    '明细项目交易费用': 'fee',\n",
    "}, inplace=True)\n",
    "\n",
    "dflabel.rename(columns={'卡号': 'card_id', '标签': 'label'}, inplace=True)\n",
    "\n",
    "df = pd.merge(df, dflabel, on='card_id', how='inner')\n",
    "\n",
    "# 将 settle_time 转为时间，方便排序\n",
    "df['settle_time'] = pd.to_datetime(df['settle_time'])\n",
    "\n",
    "print(df[['card_id', 'settle_time', '明细项目名称']].head())\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62c51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "账户数: 8917\n",
      "card_id\n",
      "00022092-02fc-45e0-83f2-c51a0d02f2d0    [拉坦前列素滴眼液, 马来酸噻吗洛尔滴眼液, 马来酸噻吗洛尔滴眼液, 芪苈强心胶囊, 参松养...\n",
      "000e9b7e-6a96-4eda-947b-425e964e1212    [银丹心脑通软胶囊, 莫匹罗星软膏, 双氯芬酸二乙胺乳胶剂, 仙灵骨葆胶囊, 麝香保心丸, ...\n",
      "000f8286-aa23-42d7-8510-2fab100bcc7b    [莫匹罗星软膏, 氨酚羟考酮片, 金水宝片, 丁丙诺啡透皮贴剂, 胞磷胆碱钠片, 非那雄胺片...\n",
      "00117f6c-e739-4913-b453-85a118a47123    [宣肺止嗽合剂, 左氧氟沙星片, 宣肺止嗽合剂, 左氧氟沙星片, 宣肺止嗽合剂, 左氧氟沙星...\n",
      "001c5c03-1db7-4303-934e-21decf219ab1    [参松养心胶囊, 麝香保心丸, 利伐沙班片, 维生素B2片, 双歧杆菌三联活菌胶囊, 胰激肽...\n",
      "Name: 明细项目名称, dtype: object\n",
      "vocab_size (含 PAD): 4120\n",
      "有效序列数: 8893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7114, 889, 890)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 构造账户级 item 序列，并建立 item 编码（含 train/val/test 划分）\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 去除缺失项目名称\n",
    "_df = df.dropna(subset=['明细项目名称']).copy()\n",
    "\n",
    "# 按 card_id 和时间排序\n",
    "_df = _df.sort_values(['card_id', 'settle_time'])\n",
    "\n",
    "# 每个账户的项目序列\n",
    "card_items = (\n",
    "    _df.groupby('card_id')['明细项目名称']\n",
    "    .apply(list)\n",
    ")\n",
    "\n",
    "print('账户数:', len(card_items))\n",
    "print(card_items.head())\n",
    "\n",
    "# 建立 item 词表：0 作为 PAD，其余从 1 开始\n",
    "all_items = sorted(set(_df['明细项目名称'].dropna().tolist()))\n",
    "item2id = {item: idx + 1 for idx, item in enumerate(all_items)}  # 1..V\n",
    "id2item = {idx: item for item, idx in item2id.items()}\n",
    "PAD_ID = 0\n",
    "vocab_size = len(item2id) + 1  # 加上 PAD\n",
    "\n",
    "print('vocab_size (含 PAD):', vocab_size)\n",
    "\n",
    "# 编码为 item id 序列\n",
    "sequences = []\n",
    "card_ids_seq = []\n",
    "\n",
    "for card_id, items in card_items.items():\n",
    "    if len(items) < 2:\n",
    "        continue\n",
    "    seq = [item2id[x] for x in items if x in item2id]\n",
    "    if len(seq) >= 2:\n",
    "        sequences.append(seq)\n",
    "        card_ids_seq.append(card_id)\n",
    "\n",
    "print('有效序列数:', len(sequences))\n",
    "\n",
    "# 按账户维度划分 train/val/test（8/1/1，可按需调整）\n",
    "indices = list(range(len(sequences)))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "train_seqs = [sequences[i] for i in train_idx]\n",
    "val_seqs = [sequences[i] for i in val_idx]\n",
    "test_seqs = [sequences[i] for i in test_idx]\n",
    "\n",
    "train_card_ids = [card_ids_seq[i] for i in train_idx]\n",
    "val_card_ids = [card_ids_seq[i] for i in val_idx]\n",
    "test_card_ids = [card_ids_seq[i] for i in test_idx]\n",
    "\n",
    "len(train_seqs), len(val_seqs), len(test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907e236c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7114, 889, 890)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 定义 Dataset / DataLoader（序列自编码：输入=输出）\n",
    "\n",
    "class SeqAEDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len=100):\n",
    "        self.max_len = max_len\n",
    "        self.sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) > max_len:\n",
    "                seq = seq[-max_len:]\n",
    "            if len(seq) >= 2:\n",
    "                self.sequences.append(seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        # 自编码：输入和目标都是完整序列\n",
    "        x = torch.tensor(seq, dtype=torch.long)\n",
    "        y = torch.tensor(seq, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 动态 padding\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    padded_inputs = torch.full((len(inputs), max_len), PAD_ID, dtype=torch.long)\n",
    "    padded_targets = torch.full((len(inputs), max_len), PAD_ID, dtype=torch.long)\n",
    "\n",
    "    for i, (inp, tgt) in enumerate(zip(inputs, targets)):\n",
    "        L = len(inp)\n",
    "        padded_inputs[i, :L] = inp\n",
    "        padded_targets[i, :L] = tgt\n",
    "\n",
    "    return padded_inputs, padded_targets, torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = SeqAEDataset(train_seqs, max_len=100)\n",
    "val_dataset = SeqAEDataset(val_seqs, max_len=100)\n",
    "test_dataset = SeqAEDataset(test_seqs, max_len=100)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce2be1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUAutoEncoder(\n",
       "  (embedding): Embedding(4120, 128, padding_idx=0)\n",
       "  (encoder_gru): GRU(128, 128, batch_first=True)\n",
       "  (decoder_gru): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=4120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 定义 GRU AutoEncoder 模型\n",
    "\n",
    "class GRUAutoEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
    "        self.encoder_gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                                  batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.decoder_gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                                  batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def encode(self, x, lengths):\n",
    "        # x: [B, T]\n",
    "        emb = self.embedding(x)  # [B, T, E]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, h = self.encoder_gru(packed)  # h: [num_layers, B, H]\n",
    "        return h\n",
    "\n",
    "    def decode(self, h, T, batch_size, device):\n",
    "        # 使用全零向量作为 decoder 输入，每一步依赖隐状态重构\n",
    "        dec_input = torch.zeros(batch_size, T, self.embedding.embedding_dim, device=device)\n",
    "        out, _ = self.decoder_gru(dec_input, h)  # [B, T, H]\n",
    "        logits = self.fc(out)  # [B, T, V]\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        B, T = x.size()\n",
    "        device = x.device\n",
    "        h = self.encode(x, lengths)\n",
    "        logits = self.decode(h, T, B, device)\n",
    "        return logits\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GRUAutoEncoder(vocab_size=vocab_size, embed_dim=128, hidden_dim=128).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a200c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss = 7.5950 | val_loss = 6.5134\n",
      "Epoch 2 | train_loss = 6.4909 | val_loss = 6.4453\n",
      "Epoch 3 | train_loss = 6.4577 | val_loss = 6.4304\n",
      "Epoch 4 | train_loss = 6.4260 | val_loss = 6.3737\n",
      "Epoch 5 | train_loss = 6.3351 | val_loss = 6.2470\n",
      "Epoch 6 | train_loss = 6.1656 | val_loss = 6.0666\n",
      "Epoch 7 | train_loss = 6.0090 | val_loss = 5.9460\n",
      "Epoch 8 | train_loss = 5.9043 | val_loss = 5.8634\n",
      "Epoch 9 | train_loss = 5.8306 | val_loss = 5.8032\n",
      "Epoch 10 | train_loss = 5.7725 | val_loss = 5.7543\n",
      "Epoch 11 | train_loss = 5.7228 | val_loss = 5.7125\n",
      "Epoch 12 | train_loss = 5.6771 | val_loss = 5.6731\n",
      "Epoch 13 | train_loss = 5.6336 | val_loss = 5.6368\n",
      "Epoch 14 | train_loss = 5.5932 | val_loss = 5.6043\n",
      "Epoch 15 | train_loss = 5.5532 | val_loss = 5.5728\n",
      "Epoch 16 | train_loss = 5.5157 | val_loss = 5.5434\n",
      "Epoch 17 | train_loss = 5.4777 | val_loss = 5.5143\n",
      "Epoch 18 | train_loss = 5.4404 | val_loss = 5.4858\n",
      "Epoch 19 | train_loss = 5.4042 | val_loss = 5.4570\n",
      "Epoch 20 | train_loss = 5.3700 | val_loss = 5.4325\n",
      "Epoch 21 | train_loss = 5.3347 | val_loss = 5.4087\n",
      "Epoch 22 | train_loss = 5.2993 | val_loss = 5.3846\n",
      "Epoch 23 | train_loss = 5.2656 | val_loss = 5.3622\n",
      "Epoch 24 | train_loss = 5.2329 | val_loss = 5.3391\n",
      "Epoch 25 | train_loss = 5.2010 | val_loss = 5.3205\n",
      "Epoch 26 | train_loss = 5.1697 | val_loss = 5.3021\n",
      "Epoch 27 | train_loss = 5.1397 | val_loss = 5.2818\n",
      "Epoch 28 | train_loss = 5.1106 | val_loss = 5.2677\n",
      "Epoch 29 | train_loss = 5.0840 | val_loss = 5.2528\n",
      "Epoch 30 | train_loss = 5.0579 | val_loss = 5.2375\n",
      "Epoch 31 | train_loss = 5.0304 | val_loss = 5.2209\n",
      "Epoch 32 | train_loss = 5.0049 | val_loss = 5.2088\n",
      "Epoch 33 | train_loss = 4.9808 | val_loss = 5.1965\n",
      "Epoch 34 | train_loss = 4.9591 | val_loss = 5.1863\n",
      "Epoch 35 | train_loss = 4.9372 | val_loss = 5.1736\n",
      "Epoch 36 | train_loss = 4.9143 | val_loss = 5.1645\n",
      "Epoch 37 | train_loss = 4.8938 | val_loss = 5.1550\n",
      "Epoch 38 | train_loss = 4.8725 | val_loss = 5.1522\n",
      "Epoch 39 | train_loss = 4.8521 | val_loss = 5.1368\n",
      "Epoch 40 | train_loss = 4.8336 | val_loss = 5.1330\n",
      "Epoch 41 | train_loss = 4.8146 | val_loss = 5.1213\n",
      "Epoch 42 | train_loss = 4.7962 | val_loss = 5.1158\n",
      "Epoch 43 | train_loss = 4.7795 | val_loss = 5.1069\n",
      "Epoch 44 | train_loss = 4.7637 | val_loss = 5.1019\n",
      "Epoch 45 | train_loss = 4.7468 | val_loss = 5.0947\n",
      "Epoch 46 | train_loss = 4.7304 | val_loss = 5.0915\n",
      "Epoch 47 | train_loss = 4.7155 | val_loss = 5.0913\n",
      "Epoch 48 | train_loss = 4.7009 | val_loss = 5.0811\n",
      "Epoch 49 | train_loss = 4.6869 | val_loss = 5.0805\n",
      "Epoch 50 | train_loss = 4.6740 | val_loss = 5.0755\n",
      "Epoch 51 | train_loss = 4.6591 | val_loss = 5.0725\n",
      "Epoch 52 | train_loss = 4.6463 | val_loss = 5.0684\n",
      "Epoch 53 | train_loss = 4.6339 | val_loss = 5.0651\n",
      "Epoch 54 | train_loss = 4.6207 | val_loss = 5.0606\n",
      "Epoch 55 | train_loss = 4.6099 | val_loss = 5.0591\n",
      "Epoch 56 | train_loss = 4.5988 | val_loss = 5.0549\n",
      "Epoch 57 | train_loss = 4.5867 | val_loss = 5.0533\n",
      "Epoch 58 | train_loss = 4.5746 | val_loss = 5.0527\n",
      "Epoch 59 | train_loss = 4.5644 | val_loss = 5.0471\n",
      "Epoch 60 | train_loss = 4.5547 | val_loss = 5.0510\n",
      "Epoch 61 | train_loss = 4.5446 | val_loss = 5.0477\n",
      "Epoch 62 | train_loss = 4.5362 | val_loss = 5.0456\n",
      "Epoch 63 | train_loss = 4.5268 | val_loss = 5.0425\n",
      "Epoch 64 | train_loss = 4.5177 | val_loss = 5.0424\n",
      "Epoch 65 | train_loss = 4.5084 | val_loss = 5.0454\n",
      "Epoch 66 | train_loss = 4.4988 | val_loss = 5.0402\n",
      "Epoch 67 | train_loss = 4.4887 | val_loss = 5.0423\n",
      "Epoch 68 | train_loss = 4.4816 | val_loss = 5.0397\n",
      "Epoch 69 | train_loss = 4.4729 | val_loss = 5.0394\n",
      "Epoch 70 | train_loss = 4.4667 | val_loss = 5.0424\n",
      "Epoch 71 | train_loss = 4.4579 | val_loss = 5.0394\n",
      "Epoch 72 | train_loss = 4.4513 | val_loss = 5.0373\n",
      "Epoch 73 | train_loss = 4.4418 | val_loss = 5.0370\n",
      "Epoch 74 | train_loss = 4.4350 | val_loss = 5.0402\n",
      "Epoch 75 | train_loss = 4.4287 | val_loss = 5.0393\n",
      "Epoch 76 | train_loss = 4.4203 | val_loss = 5.0418\n",
      "Epoch 77 | train_loss = 4.4155 | val_loss = 5.0375\n",
      "Epoch 78 | train_loss = 4.4111 | val_loss = 5.0380\n",
      "Epoch 79 | train_loss = 4.4039 | val_loss = 5.0416\n",
      "Epoch 80 | train_loss = 4.3970 | val_loss = 5.0385\n",
      "Epoch 81 | train_loss = 4.3896 | val_loss = 5.0416\n",
      "Epoch 82 | train_loss = 4.3821 | val_loss = 5.0401\n",
      "Epoch 83 | train_loss = 4.3768 | val_loss = 5.0427\n",
      "Epoch 84 | train_loss = 4.3713 | val_loss = 5.0409\n",
      "Epoch 85 | train_loss = 4.3659 | val_loss = 5.0427\n",
      "Epoch 86 | train_loss = 4.3592 | val_loss = 5.0447\n",
      "Epoch 87 | train_loss = 4.3527 | val_loss = 5.0442\n",
      "Epoch 88 | train_loss = 4.3475 | val_loss = 5.0523\n",
      "Epoch 89 | train_loss = 4.3438 | val_loss = 5.0487\n",
      "Epoch 90 | train_loss = 4.3384 | val_loss = 5.0469\n",
      "Epoch 91 | train_loss = 4.3328 | val_loss = 5.0487\n",
      "Epoch 92 | train_loss = 4.3311 | val_loss = 5.0435\n",
      "Epoch 93 | train_loss = 4.3295 | val_loss = 5.0510\n",
      "Epoch 94 | train_loss = 4.3207 | val_loss = 5.0510\n",
      "Epoch 95 | train_loss = 4.3141 | val_loss = 5.0496\n",
      "Epoch 96 | train_loss = 4.3077 | val_loss = 5.0569\n",
      "Epoch 97 | train_loss = 4.3037 | val_loss = 5.0533\n",
      "Epoch 98 | train_loss = 4.2997 | val_loss = 5.0512\n",
      "Epoch 99 | train_loss = 4.2971 | val_loss = 5.0553\n",
      "Epoch 100 | train_loss = 4.2932 | val_loss = 5.0596\n",
      "Epoch 101 | train_loss = 4.2895 | val_loss = 5.0593\n",
      "Epoch 102 | train_loss = 4.2887 | val_loss = 5.0605\n",
      "Epoch 103 | train_loss = 4.2832 | val_loss = 5.0636\n",
      "Early stopping at epoch 103 (no val improvement for 30 epochs).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finished'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 训练 GRU AutoEncoder，并在验证集上 Early Stopping\n",
    "\n",
    "import copy\n",
    "\n",
    "num_epochs = 200  # 最大训练轮数\n",
    "patience = 30     # early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------- 训练 ----------\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, targets, lengths = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs, lengths)   # [B, T, V]\n",
    "\n",
    "        B, T, V = logits.shape\n",
    "        loss = criterion(logits.view(B * T, V), targets.view(B * T))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        total_steps += B\n",
    "\n",
    "    train_avg_loss = total_loss / max(total_steps, 1)\n",
    "\n",
    "    # ---------- 验证 ----------\n",
    "    model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    val_total_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, targets, lengths = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            logits = model(inputs, lengths)\n",
    "            B, T, V = logits.shape\n",
    "            loss = criterion(logits.view(B * T, V), targets.view(B * T))\n",
    "\n",
    "            val_total_loss += loss.item() * B\n",
    "            val_total_steps += B\n",
    "\n",
    "    val_avg_loss = val_total_loss / max(val_total_steps, 1)\n",
    "\n",
    "    print(f\"Epoch {epoch} | train_loss = {train_avg_loss:.4f} | val_loss = {val_avg_loss:.4f}\")\n",
    "\n",
    "    # ---------- Early Stopping & 保存最优模型 ----------\n",
    "    if val_avg_loss < best_val_loss:\n",
    "        best_val_loss = val_avg_loss\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        torch.save({'model_state_dict': best_state, 'item2id': item2id}, 'gru_ae_model_best.pt')\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no val improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "# 训练结束后，加载最佳验证损失对应的权重\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792b9384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 5.007492749351847)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 利用重构误差进行无监督异常检测（账户级），在测试集上评估\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "card_step_losses = {}\n",
    "step_losses_all = []\n",
    "step_recalls_all = []\n",
    "step_ndcgs_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq, card_id in zip(test_seqs, test_card_ids):\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        # 截断到与训练相同的 max_len\n",
    "        if len(seq) > 100:\n",
    "            seq_use = seq[-100:]\n",
    "        else:\n",
    "            seq_use = seq\n",
    "\n",
    "        inp = torch.tensor(seq_use, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "        tgt = torch.tensor(seq_use, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "        lengths = torch.tensor([inp.size(1)], dtype=torch.long, device=device)\n",
    "\n",
    "        logits = model(inp, lengths)  # [1, T, V]\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)  # [1, T, V]\n",
    "\n",
    "        T = tgt.size(1)\n",
    "        for t in range(T):\n",
    "            true_id = tgt[0, t].item()\n",
    "            if true_id == PAD_ID:\n",
    "                continue\n",
    "\n",
    "            lp = log_probs[0, t, true_id].item()\n",
    "            loss_t = -lp\n",
    "            step_losses_all.append(loss_t)\n",
    "\n",
    "            # 排名：按概率排序，计算 Recall@10 和 NDCG@10\n",
    "            probs_t = log_probs[0, t].exp().cpu().numpy()  # 转回概率\n",
    "            ranked_ids = np.argsort(-probs_t)\n",
    "\n",
    "            rank = np.where(ranked_ids == true_id)[0]\n",
    "            if len(rank) > 0:\n",
    "                rank = int(rank[0]) + 1  # 从 1 开始\n",
    "            else:\n",
    "                rank = None\n",
    "\n",
    "            if rank is not None and rank <= 10:\n",
    "                step_recalls_all.append(1.0)\n",
    "                step_ndcgs_all.append(1.0 / math.log2(rank + 1))\n",
    "            else:\n",
    "                step_recalls_all.append(0.0)\n",
    "                step_ndcgs_all.append(0.0)\n",
    "\n",
    "            card_step_losses.setdefault(card_id, []).append(loss_t)\n",
    "\n",
    "# 账户级异常分数：平均 loss（越大越异常）\n",
    "card_ids_scored = []\n",
    "card_scores = []\n",
    "card_steps = []\n",
    "\n",
    "for cid, losses in card_step_losses.items():\n",
    "    card_ids_scored.append(cid)\n",
    "    card_scores.append(float(np.mean(losses)))\n",
    "    card_steps.append(len(losses))\n",
    "\n",
    "len(card_ids_scored), np.mean(card_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3ffb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.47287055162375147,\n",
       " 'PR_AUC': 0.21656374834309702,\n",
       " 'Precision': 0.19662921348314608,\n",
       " 'Recall': 0.1724137931034483,\n",
       " 'F1': 0.1837270341207349}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. 账户级结果整理与评估（AUC、PR-AUC、Precision、Recall、F1）\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'card_id': card_ids_scored,\n",
    "    'gru_ae_score': card_scores,   # 平均重构 loss，越大越异常\n",
    "    'num_steps': card_steps,\n",
    "})\n",
    "\n",
    "# 简单策略：使用 99 分位数作为异常阈值（可根据需要调整）\n",
    "threshold = result_df['gru_ae_score'].quantile(0.8)\n",
    "result_df['gru_ae_label'] = (result_df['gru_ae_score'] >= threshold).astype(int)  # 1=异常, 0=正常\n",
    "\n",
    "# 账户级真实标签：按 card_id 聚合明细标签，这里用 max 规则（账户内只要有一条是 1，就认为账户为 1）\n",
    "card_label = (\n",
    "    df.groupby('card_id')['label']\n",
    "    .max()\n",
    "    .reindex(result_df['card_id'])  # 按 result_df 对齐\n",
    ")\n",
    "\n",
    "# 转成 numpy 数组\n",
    "y_true = card_label.values.astype(int)\n",
    "\n",
    "# gru_ae_score 本身就是“越大越异常”，可以直接作为异常分数\n",
    "anomaly_prob = result_df['gru_ae_score'].values\n",
    "\n",
    "# AUC-ROC\n",
    "auc = roc_auc_score(y_true, anomaly_prob)\n",
    "\n",
    "# PR-AUC（Average Precision）\n",
    "pr_auc = average_precision_score(y_true, anomaly_prob)\n",
    "\n",
    "# 二值预测：使用 gru_ae_label（1=异常，0=正常）\n",
    "y_pred = result_df['gru_ae_label'].values\n",
    "\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "metrics = {\n",
    "    'AUC': auc,\n",
    "    'PR_AUC': pr_auc,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1': f1,\n",
    "}\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5149faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Avg_Loss': 5.092228185085173,\n",
       " 'Recall@10': 0.3518897084076079,\n",
       " 'NDCG@10': 0.1923805126704318}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. 序列级指标：Avg_Loss、Recall@10、NDCG@10\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "avg_loss = float(np.mean(step_losses_all)) if step_losses_all else float('nan')\n",
    "recall_at_10 = float(np.mean(step_recalls_all)) if step_recalls_all else float('nan')\n",
    "ndcg_at_10 = float(np.mean(step_ndcgs_all)) if step_ndcgs_all else float('nan')\n",
    "\n",
    "seq_metrics = {\n",
    "    'Avg_Loss': avg_loss,\n",
    "    'Recall@10': recall_at_10,\n",
    "    'NDCG@10': ndcg_at_10,\n",
    "}\n",
    "\n",
    "seq_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4642bd-4f8a-47d3-bf8a-4732ab3e60ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad_env",
   "language": "python",
   "name": "gad_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
