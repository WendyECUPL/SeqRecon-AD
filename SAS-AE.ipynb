{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc70fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                card_id         settle_time       明细项目名称\n",
      "0  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48        尼可地尔片\n",
      "1  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48  头孢克洛缓释片(II)\n",
      "2  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48      乳果糖口服溶液\n",
      "3  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48        艾司唑仑片\n",
      "4  dccc6fc4-d367-420f-846d-ef5ece5cc1d2 2023-03-13 09:43:49    盐酸地尔硫卓缓释片\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1107985"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 导入依赖 & 读取数据\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 载入明细和标签，字段名与 featureprocessing-Copy1.ipynb 保持一致\n",
    "df = pd.read_csv('model1_data.csv', encoding='gbk', low_memory=False)\n",
    "dflabel = pd.read_csv('model1_label.csv', encoding='gbk')\n",
    "\n",
    "# 重命名字段并合并标签\n",
    "df.rename(columns={\n",
    "    '卡号': 'card_id',\n",
    "    '机构名称': 'org_id',\n",
    "    '结算日期时间': 'settle_time',\n",
    "    '明细项目交易费用': 'fee',\n",
    "}, inplace=True)\n",
    "\n",
    "dflabel.rename(columns={'卡号': 'card_id', '标签': 'label'}, inplace=True)\n",
    "\n",
    "df = pd.merge(df, dflabel, on='card_id', how='inner')\n",
    "\n",
    "# 将 settle_time 转为时间，方便排序\n",
    "df['settle_time'] = pd.to_datetime(df['settle_time'])\n",
    "\n",
    "print(df[['card_id', 'settle_time', '明细项目名称']].head())\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fa1247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "账户数: 8917\n",
      "card_id\n",
      "00022092-02fc-45e0-83f2-c51a0d02f2d0    [拉坦前列素滴眼液, 马来酸噻吗洛尔滴眼液, 马来酸噻吗洛尔滴眼液, 芪苈强心胶囊, 参松养...\n",
      "000e9b7e-6a96-4eda-947b-425e964e1212    [银丹心脑通软胶囊, 莫匹罗星软膏, 双氯芬酸二乙胺乳胶剂, 仙灵骨葆胶囊, 麝香保心丸, ...\n",
      "000f8286-aa23-42d7-8510-2fab100bcc7b    [莫匹罗星软膏, 氨酚羟考酮片, 金水宝片, 丁丙诺啡透皮贴剂, 胞磷胆碱钠片, 非那雄胺片...\n",
      "00117f6c-e739-4913-b453-85a118a47123    [宣肺止嗽合剂, 左氧氟沙星片, 宣肺止嗽合剂, 左氧氟沙星片, 宣肺止嗽合剂, 左氧氟沙星...\n",
      "001c5c03-1db7-4303-934e-21decf219ab1    [参松养心胶囊, 麝香保心丸, 利伐沙班片, 维生素B2片, 双歧杆菌三联活菌胶囊, 胰激肽...\n",
      "Name: 明细项目名称, dtype: object\n",
      "vocab_size (含 PAD): 4120\n",
      "有效序列数: 8893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7114, 889, 890)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 构造账户级 item 序列，并建立 item 编码（含 train/val/test 划分）\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 去除缺失项目名称\n",
    "_df = df.dropna(subset=['明细项目名称']).copy()\n",
    "\n",
    "# 按 card_id 和时间排序\n",
    "_df = _df.sort_values(['card_id', 'settle_time'])\n",
    "\n",
    "# 每个账户的项目序列\n",
    "card_items = (\n",
    "    _df.groupby('card_id')['明细项目名称']\n",
    "    .apply(list)\n",
    ")\n",
    "\n",
    "print('账户数:', len(card_items))\n",
    "print(card_items.head())\n",
    "\n",
    "# 建立 item 词表：0 作为 PAD，其余从 1 开始\n",
    "all_items = sorted(set(_df['明细项目名称'].dropna().tolist()))\n",
    "item2id = {item: idx + 1 for idx, item in enumerate(all_items)}  # 1..V\n",
    "id2item = {idx: item for item, idx in item2id.items()}\n",
    "PAD_ID = 0\n",
    "vocab_size = len(item2id) + 1  # 加上 PAD\n",
    "\n",
    "print('vocab_size (含 PAD):', vocab_size)\n",
    "\n",
    "# 编码为 item id 序列\n",
    "sequences = []\n",
    "card_ids_seq = []\n",
    "\n",
    "for card_id, items in card_items.items():\n",
    "    if len(items) < 2:\n",
    "        continue\n",
    "    seq = [item2id[x] for x in items if x in item2id]\n",
    "    if len(seq) >= 2:\n",
    "        sequences.append(seq)\n",
    "        card_ids_seq.append(card_id)\n",
    "\n",
    "print('有效序列数:', len(sequences))\n",
    "\n",
    "# 按账户维度划分 train/val/test（8/1/1，可按需调整）\n",
    "indices = list(range(len(sequences)))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "train_seqs = [sequences[i] for i in train_idx]\n",
    "val_seqs = [sequences[i] for i in val_idx]\n",
    "test_seqs = [sequences[i] for i in test_idx]\n",
    "\n",
    "train_card_ids = [card_ids_seq[i] for i in train_idx]\n",
    "val_card_ids = [card_ids_seq[i] for i in val_idx]\n",
    "test_card_ids = [card_ids_seq[i] for i in test_idx]\n",
    "\n",
    "len(train_seqs), len(val_seqs), len(test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca967984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7114, 889, 890)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 定义 Dataset / DataLoader（序列自编码：输入=输出）\n",
    "\n",
    "class SeqAEDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len=100):\n",
    "        self.max_len = max_len\n",
    "        self.sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) > max_len:\n",
    "                seq = seq[-max_len:]\n",
    "            if len(seq) >= 2:\n",
    "                self.sequences.append(seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        x = torch.tensor(seq, dtype=torch.long)\n",
    "        y = torch.tensor(seq, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 动态 padding\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    padded_inputs = torch.full((len(inputs), max_len), PAD_ID, dtype=torch.long)\n",
    "    padded_targets = torch.full((len(inputs), max_len), PAD_ID, dtype=torch.long)\n",
    "\n",
    "    for i, (inp, tgt) in enumerate(zip(inputs, targets)):\n",
    "        L = len(inp)\n",
    "        padded_inputs[i, :L] = inp\n",
    "        padded_targets[i, :L] = tgt\n",
    "\n",
    "    return padded_inputs, padded_targets, torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = SeqAEDataset(train_seqs, max_len=100)\n",
    "val_dataset = SeqAEDataset(val_seqs, max_len=100)\n",
    "test_dataset = SeqAEDataset(test_seqs, max_len=100)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7e1ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SASAE(\n",
       "  (embedding): Embedding(4120, 128, padding_idx=0)\n",
       "  (pos_embedding): Embedding(100, 128)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=4120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 定义 SAS-AE 模型（基于自注意力的自编码器）\n",
    "\n",
    "class SASAE(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len=100, embed_dim=128, num_heads=4,\n",
    "                 num_layers=2, ff_dim=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 使用一个 TransformerEncoder 作为解码器的近似（输入为位置嵌入 + 编码向量）\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T]\n",
    "        B, T = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        positions = torch.arange(T, device=device).unsqueeze(0).expand(B, T)\n",
    "        x_emb = self.embedding(x) + self.pos_embedding(positions)  # [B, T, E]\n",
    "\n",
    "        pad_mask = (x == PAD_ID)  # [B, T]\n",
    "\n",
    "        # 编码\n",
    "        enc_out = self.encoder(x_emb, src_key_padding_mask=pad_mask)  # [B, T, E]\n",
    "\n",
    "        # 使用编码结果的均值作为压缩表示（全局表示）\n",
    "        enc_repr = (enc_out * (~pad_mask).unsqueeze(-1)).sum(dim=1) / ((~pad_mask).sum(dim=1, keepdim=True) + 1e-8)\n",
    "        # 扩展到整个序列长度作为解码输入\n",
    "        dec_input = enc_repr.unsqueeze(1).expand(B, T, enc_out.size(-1)) + self.pos_embedding(positions)\n",
    "\n",
    "        dec_out = self.decoder(dec_input, src_key_padding_mask=pad_mask)  # [B, T, E]\n",
    "        logits = self.fc(dec_out)  # [B, T, V]\n",
    "        return logits\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SASAE(vocab_size=vocab_size, max_len=100, embed_dim=128).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38713006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/envs/gad_env/lib/python3.9/site-packages/torch/nn/modules/transformer.py:409: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss = 7.1228 | val_loss = 6.4589\n",
      "Epoch 2 | train_loss = 6.3371 | val_loss = 6.0521\n",
      "Epoch 3 | train_loss = 5.9459 | val_loss = 5.8308\n",
      "Epoch 4 | train_loss = 5.7983 | val_loss = 5.7427\n",
      "Epoch 5 | train_loss = 5.7076 | val_loss = 5.6660\n",
      "Epoch 6 | train_loss = 5.6296 | val_loss = 5.6010\n",
      "Epoch 7 | train_loss = 5.5553 | val_loss = 5.5397\n",
      "Epoch 8 | train_loss = 5.4825 | val_loss = 5.4832\n",
      "Epoch 9 | train_loss = 5.4125 | val_loss = 5.4279\n",
      "Epoch 10 | train_loss = 5.3428 | val_loss = 5.3747\n",
      "Epoch 11 | train_loss = 5.2757 | val_loss = 5.3282\n",
      "Epoch 12 | train_loss = 5.2104 | val_loss = 5.2844\n",
      "Epoch 13 | train_loss = 5.1470 | val_loss = 5.2448\n",
      "Epoch 14 | train_loss = 5.0892 | val_loss = 5.2094\n",
      "Epoch 15 | train_loss = 5.0341 | val_loss = 5.1735\n",
      "Epoch 16 | train_loss = 4.9835 | val_loss = 5.1452\n",
      "Epoch 17 | train_loss = 4.9370 | val_loss = 5.1200\n",
      "Epoch 18 | train_loss = 4.8893 | val_loss = 5.0938\n",
      "Epoch 19 | train_loss = 4.8460 | val_loss = 5.0676\n",
      "Epoch 20 | train_loss = 4.8049 | val_loss = 5.0514\n",
      "Epoch 21 | train_loss = 4.7659 | val_loss = 5.0353\n",
      "Epoch 22 | train_loss = 4.7288 | val_loss = 5.0179\n",
      "Epoch 23 | train_loss = 4.6937 | val_loss = 5.0027\n",
      "Epoch 24 | train_loss = 4.6606 | val_loss = 4.9919\n",
      "Epoch 25 | train_loss = 4.6287 | val_loss = 4.9767\n",
      "Epoch 26 | train_loss = 4.5996 | val_loss = 4.9670\n",
      "Epoch 27 | train_loss = 4.5679 | val_loss = 4.9595\n",
      "Epoch 28 | train_loss = 4.5412 | val_loss = 4.9462\n",
      "Epoch 29 | train_loss = 4.5151 | val_loss = 4.9402\n",
      "Epoch 30 | train_loss = 4.4896 | val_loss = 4.9378\n",
      "Epoch 31 | train_loss = 4.4639 | val_loss = 4.9308\n",
      "Epoch 32 | train_loss = 4.4405 | val_loss = 4.9196\n",
      "Epoch 33 | train_loss = 4.4166 | val_loss = 4.9185\n",
      "Epoch 34 | train_loss = 4.3960 | val_loss = 4.9083\n",
      "Epoch 35 | train_loss = 4.3732 | val_loss = 4.9085\n",
      "Epoch 36 | train_loss = 4.3531 | val_loss = 4.9032\n",
      "Epoch 37 | train_loss = 4.3329 | val_loss = 4.9025\n",
      "Epoch 38 | train_loss = 4.3134 | val_loss = 4.9041\n",
      "Epoch 39 | train_loss = 4.2957 | val_loss = 4.8977\n",
      "Epoch 40 | train_loss = 4.2772 | val_loss = 4.8949\n",
      "Epoch 41 | train_loss = 4.2585 | val_loss = 4.8951\n",
      "Epoch 42 | train_loss = 4.2428 | val_loss = 4.8918\n",
      "Epoch 43 | train_loss = 4.2251 | val_loss = 4.8915\n",
      "Epoch 44 | train_loss = 4.2087 | val_loss = 4.8958\n",
      "Epoch 45 | train_loss = 4.1949 | val_loss = 4.8980\n",
      "Epoch 46 | train_loss = 4.1791 | val_loss = 4.8934\n",
      "Epoch 47 | train_loss = 4.1630 | val_loss = 4.8936\n",
      "Epoch 48 | train_loss = 4.1483 | val_loss = 4.8967\n",
      "Epoch 49 | train_loss = 4.1340 | val_loss = 4.8971\n",
      "Epoch 50 | train_loss = 4.1214 | val_loss = 4.8976\n",
      "Epoch 51 | train_loss = 4.1074 | val_loss = 4.9008\n",
      "Epoch 52 | train_loss = 4.0927 | val_loss = 4.8994\n",
      "Epoch 53 | train_loss = 4.0811 | val_loss = 4.9022\n",
      "Epoch 54 | train_loss = 4.0679 | val_loss = 4.9049\n",
      "Epoch 55 | train_loss = 4.0561 | val_loss = 4.9045\n",
      "Epoch 56 | train_loss = 4.0431 | val_loss = 4.9107\n",
      "Epoch 57 | train_loss = 4.0314 | val_loss = 4.9120\n",
      "Epoch 58 | train_loss = 4.0210 | val_loss = 4.9161\n",
      "Epoch 59 | train_loss = 4.0112 | val_loss = 4.9191\n",
      "Epoch 60 | train_loss = 3.9999 | val_loss = 4.9213\n",
      "Epoch 61 | train_loss = 3.9900 | val_loss = 4.9245\n",
      "Epoch 62 | train_loss = 3.9796 | val_loss = 4.9266\n",
      "Epoch 63 | train_loss = 3.9684 | val_loss = 4.9311\n",
      "Epoch 64 | train_loss = 3.9601 | val_loss = 4.9365\n",
      "Epoch 65 | train_loss = 3.9492 | val_loss = 4.9368\n",
      "Epoch 66 | train_loss = 3.9395 | val_loss = 4.9388\n",
      "Epoch 67 | train_loss = 3.9310 | val_loss = 4.9409\n",
      "Epoch 68 | train_loss = 3.9215 | val_loss = 4.9413\n",
      "Epoch 69 | train_loss = 3.9105 | val_loss = 4.9466\n",
      "Epoch 70 | train_loss = 3.9023 | val_loss = 4.9495\n",
      "Epoch 71 | train_loss = 3.8951 | val_loss = 4.9514\n",
      "Epoch 72 | train_loss = 3.8855 | val_loss = 4.9556\n",
      "Epoch 73 | train_loss = 3.8776 | val_loss = 4.9582\n",
      "Early stopping at epoch 73 (no val improvement for 30 epochs).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finished'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 训练 SAS-AE，并在验证集上 Early Stopping\n",
    "\n",
    "import copy\n",
    "\n",
    "num_epochs = 200  # 最大训练轮数\n",
    "patience = 30     # early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------- 训练 ----------\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, targets, lengths = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)   # [B, T, V]\n",
    "\n",
    "        B, T, V = logits.shape\n",
    "        loss = criterion(logits.view(B * T, V), targets.view(B * T))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        total_steps += B\n",
    "\n",
    "    train_avg_loss = total_loss / max(total_steps, 1)\n",
    "\n",
    "    # ---------- 验证 ----------\n",
    "    model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    val_total_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, targets, lengths = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            B, T, V = logits.shape\n",
    "            loss = criterion(logits.view(B * T, V), targets.view(B * T))\n",
    "\n",
    "            val_total_loss += loss.item() * B\n",
    "            val_total_steps += B\n",
    "\n",
    "    val_avg_loss = val_total_loss / max(val_total_steps, 1)\n",
    "\n",
    "    print(f\"Epoch {epoch} | train_loss = {train_avg_loss:.4f} | val_loss = {val_avg_loss:.4f}\")\n",
    "\n",
    "    # ---------- Early Stopping & 保存最优模型 ----------\n",
    "    if val_avg_loss < best_val_loss:\n",
    "        best_val_loss = val_avg_loss\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        torch.save({'model_state_dict': best_state, 'item2id': item2id}, 'sas_ae_model_best.pt')\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no val improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "# 训练结束后，加载最佳验证损失对应的权重\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bd5ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 4.820091507571927)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 利用重构误差进行无监督异常检测（账户级），在测试集上评估\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "card_step_losses = {}\n",
    "step_losses_all = []\n",
    "step_recalls_all = []\n",
    "step_ndcgs_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq, card_id in zip(test_seqs, test_card_ids):\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        # 截断到与训练相同的 max_len\n",
    "        if len(seq) > 100:\n",
    "            seq_use = seq[-100:]\n",
    "        else:\n",
    "            seq_use = seq\n",
    "\n",
    "        inp = torch.tensor(seq_use, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "        tgt = torch.tensor(seq_use, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "\n",
    "        logits = model(inp)  # [1, T, V]\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)  # [1, T, V]\n",
    "\n",
    "        T = tgt.size(1)\n",
    "        for t in range(T):\n",
    "            true_id = tgt[0, t].item()\n",
    "            if true_id == PAD_ID:\n",
    "                continue\n",
    "\n",
    "            lp = log_probs[0, t, true_id].item()\n",
    "            loss_t = -lp\n",
    "            step_losses_all.append(loss_t)\n",
    "\n",
    "            # 排名：按概率排序，计算 Recall@10 和 NDCG@10\n",
    "            probs_t = log_probs[0, t].exp().cpu().numpy()  # 转回概率\n",
    "            ranked_ids = np.argsort(-probs_t)\n",
    "\n",
    "            rank = np.where(ranked_ids == true_id)[0]\n",
    "            if len(rank) > 0:\n",
    "                rank = int(rank[0]) + 1  # 从 1 开始\n",
    "            else:\n",
    "                rank = None\n",
    "\n",
    "            if rank is not None and rank <= 10:\n",
    "                step_recalls_all.append(1.0)\n",
    "                step_ndcgs_all.append(1.0 / math.log2(rank + 1))\n",
    "            else:\n",
    "                step_recalls_all.append(0.0)\n",
    "                step_ndcgs_all.append(0.0)\n",
    "\n",
    "            card_step_losses.setdefault(card_id, []).append(loss_t)\n",
    "\n",
    "# 账户级异常分数：平均 loss（越大越异常）\n",
    "card_ids_scored = []\n",
    "card_scores = []\n",
    "card_steps = []\n",
    "\n",
    "for cid, losses in card_step_losses.items():\n",
    "    card_ids_scored.append(cid)\n",
    "    card_scores.append(float(np.mean(losses)))\n",
    "    card_steps.append(len(losses))\n",
    "\n",
    "len(card_ids_scored), np.mean(card_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7fed31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.5015165530148213,\n",
       " 'PR_AUC': 0.2296373919996623,\n",
       " 'Precision': 0.2303370786516854,\n",
       " 'Recall': 0.2019704433497537,\n",
       " 'F1': 0.2152230971128609}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. 账户级结果整理与评估（AUC、PR-AUC、Precision、Recall、F1）\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'card_id': card_ids_scored,\n",
    "    'sas_ae_score': card_scores,   # 平均重构 loss，越大越异常\n",
    "    'num_steps': card_steps,\n",
    "})\n",
    "\n",
    "# 简单策略：使用 99 分位数作为异常阈值（可根据需要调整）\n",
    "threshold = result_df['sas_ae_score'].quantile(0.8)\n",
    "result_df['sas_ae_label'] = (result_df['sas_ae_score'] >= threshold).astype(int)  # 1=异常, 0=正常\n",
    "\n",
    "# 账户级真实标签：按 card_id 聚合明细标签，这里用 max 规则（账户内只要有一条是 1，就认为账户为 1）\n",
    "card_label = (\n",
    "    df.groupby('card_id')['label']\n",
    "    .max()\n",
    "    .reindex(result_df['card_id'])  # 按 result_df 对齐\n",
    ")\n",
    "\n",
    "# 转成 numpy 数组\n",
    "y_true = card_label.values.astype(int)\n",
    "\n",
    "# sas_ae_score 本身就是“越大越异常”，可以直接作为异常分数\n",
    "anomaly_prob = result_df['sas_ae_score'].values\n",
    "\n",
    "# AUC-ROC\n",
    "auc = roc_auc_score(y_true, anomaly_prob)\n",
    "\n",
    "# PR-AUC（Average Precision）\n",
    "pr_auc = average_precision_score(y_true, anomaly_prob)\n",
    "\n",
    "# 二值预测：使用 sas_ae_label（1=异常，0=正常）\n",
    "y_pred = result_df['sas_ae_label'].values\n",
    "\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "metrics = {\n",
    "    'AUC': auc,\n",
    "    'PR_AUC': pr_auc,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1': f1,\n",
    "}\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614a1381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Avg_Loss': 4.945391828033666,\n",
       " 'Recall@10': 0.39635632165110307,\n",
       " 'NDCG@10': 0.21413278456769091}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. 序列级指标：Avg_Loss、Recall@10、NDCG@10\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "avg_loss = float(np.mean(step_losses_all)) if step_losses_all else float('nan')\n",
    "recall_at_10 = float(np.mean(step_recalls_all)) if step_recalls_all else float('nan')\n",
    "ndcg_at_10 = float(np.mean(step_ndcgs_all)) if step_ndcgs_all else float('nan')\n",
    "\n",
    "seq_metrics = {\n",
    "    'Avg_Loss': avg_loss,\n",
    "    'Recall@10': recall_at_10,\n",
    "    'NDCG@10': ndcg_at_10,\n",
    "}\n",
    "\n",
    "seq_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad_env",
   "language": "python",
   "name": "gad_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
