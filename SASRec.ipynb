{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb95bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                card_id         settle_time       明细项目名称\n",
      "0  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48        尼可地尔片\n",
      "1  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48  头孢克洛缓释片(II)\n",
      "2  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48      乳果糖口服溶液\n",
      "3  ff88846b-56ec-4d2f-a2fc-aca3c116c865 2023-03-29 09:18:48        艾司唑仑片\n",
      "4  dccc6fc4-d367-420f-846d-ef5ece5cc1d2 2023-03-13 09:43:49    盐酸地尔硫卓缓释片\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1107985"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 导入依赖 & 读取数据\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 载入明细和标签，字段名与 featureprocessing-Copy1.ipynb 保持一致\n",
    "df = pd.read_csv('model1_data.csv', encoding='gbk', low_memory=False)\n",
    "dflabel = pd.read_csv('model1_label.csv', encoding='gbk')\n",
    "\n",
    "# 重命名字段并合并标签\n",
    "df.rename(columns={\n",
    "    '卡号': 'card_id',\n",
    "    '机构名称': 'org_id',\n",
    "    '结算日期时间': 'settle_time',\n",
    "    '明细项目交易费用': 'fee',\n",
    "}, inplace=True)\n",
    "\n",
    "dflabel.rename(columns={'卡号': 'card_id', '标签': 'label'}, inplace=True)\n",
    "\n",
    "df = pd.merge(df, dflabel, on='card_id', how='inner')\n",
    "\n",
    "# 将 settle_time 转为时间，方便排序\n",
    "df['settle_time'] = pd.to_datetime(df['settle_time'])\n",
    "\n",
    "print(df[['card_id', 'settle_time', '明细项目名称']].head())\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113efcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "账户数: 8917\n",
      "card_id\n",
      "00022092-02fc-45e0-83f2-c51a0d02f2d0    [拉坦前列素滴眼液, 马来酸噻吗洛尔滴眼液, 马来酸噻吗洛尔滴眼液, 芪苈强心胶囊, 参松养...\n",
      "000e9b7e-6a96-4eda-947b-425e964e1212    [银丹心脑通软胶囊, 莫匹罗星软膏, 双氯芬酸二乙胺乳胶剂, 仙灵骨葆胶囊, 麝香保心丸, ...\n",
      "000f8286-aa23-42d7-8510-2fab100bcc7b    [莫匹罗星软膏, 氨酚羟考酮片, 金水宝片, 丁丙诺啡透皮贴剂, 胞磷胆碱钠片, 非那雄胺片...\n",
      "00117f6c-e739-4913-b453-85a118a47123    [宣肺止嗽合剂, 左氧氟沙星片, 宣肺止嗽合剂, 左氧氟沙星片, 宣肺止嗽合剂, 左氧氟沙星...\n",
      "001c5c03-1db7-4303-934e-21decf219ab1    [参松养心胶囊, 麝香保心丸, 利伐沙班片, 维生素B2片, 双歧杆菌三联活菌胶囊, 胰激肽...\n",
      "Name: 明细项目名称, dtype: object\n",
      "vocab_size (含 PAD): 4120\n",
      "有效序列数: 8893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7114, 889, 890)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 构造账户级 item 序列，并建立 item 编码（含 train/val/test 划分）\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 去除缺失项目名称\n",
    "_df = df.dropna(subset=['明细项目名称']).copy()\n",
    "\n",
    "# 按 card_id 和时间排序\n",
    "_df = _df.sort_values(['card_id', 'settle_time'])\n",
    "\n",
    "# 每个账户的项目序列\n",
    "card_items = (\n",
    "    _df.groupby('card_id')['明细项目名称']\n",
    "    .apply(list)\n",
    ")\n",
    "\n",
    "print('账户数:', len(card_items))\n",
    "print(card_items.head())\n",
    "\n",
    "# 建立 item 词表：0 作为 PAD，其余从 1 开始\n",
    "all_items = sorted(set(_df['明细项目名称'].dropna().tolist()))\n",
    "item2id = {item: idx + 1 for idx, item in enumerate(all_items)}  # 1..V\n",
    "id2item = {idx: item for item, idx in item2id.items()}\n",
    "PAD_ID = 0\n",
    "vocab_size = len(item2id) + 1  # 加上 PAD\n",
    "\n",
    "print('vocab_size (含 PAD):', vocab_size)\n",
    "\n",
    "# 编码为 item id 序列\n",
    "sequences = []\n",
    "card_ids_seq = []\n",
    "\n",
    "for card_id, items in card_items.items():\n",
    "    if len(items) < 2:\n",
    "        continue\n",
    "    seq = [item2id[x] for x in items if x in item2id]\n",
    "    if len(seq) >= 2:\n",
    "        sequences.append(seq)\n",
    "        card_ids_seq.append(card_id)\n",
    "\n",
    "print('有效序列数:', len(sequences))\n",
    "\n",
    "# 按账户维度划分 train/val/test（8/1/1，可按需调整）\n",
    "indices = list(range(len(sequences)))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "train_seqs = [sequences[i] for i in train_idx]\n",
    "val_seqs = [sequences[i] for i in val_idx]\n",
    "test_seqs = [sequences[i] for i in test_idx]\n",
    "\n",
    "train_card_ids = [card_ids_seq[i] for i in train_idx]\n",
    "val_card_ids = [card_ids_seq[i] for i in val_idx]\n",
    "test_card_ids = [card_ids_seq[i] for i in test_idx]\n",
    "\n",
    "len(train_seqs), len(val_seqs), len(test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0236027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7114, 889, 890)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 定义 Dataset / DataLoader，用于 next-item prediction 训练（带 train/val/test）\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len=100):\n",
    "        self.max_len = max_len\n",
    "        self.sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) > max_len:\n",
    "                seq = seq[-max_len:]\n",
    "            if len(seq) >= 2:\n",
    "                self.sequences.append(seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        # 输入为前 n-1，目标为后 n-1\n",
    "        return torch.tensor(seq[:-1], dtype=torch.long), torch.tensor(seq[1:], dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 动态 padding\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    padded_inputs = torch.full((len(inputs), max_len), PAD_ID, dtype=torch.long)\n",
    "    padded_targets = torch.full((len(inputs), max_len), PAD_ID, dtype=torch.long)\n",
    "\n",
    "    for i, (inp, tgt) in enumerate(zip(inputs, targets)):\n",
    "        L = len(inp)\n",
    "        padded_inputs[i, :L] = inp\n",
    "        padded_targets[i, :L] = tgt\n",
    "\n",
    "    return padded_inputs, padded_targets, torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = SeqDataset(train_seqs, max_len=100)\n",
    "val_dataset = SeqDataset(val_seqs, max_len=100)\n",
    "test_dataset = SeqDataset(test_seqs, max_len=100)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488d058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SASRec(\n",
       "  (embedding): Embedding(4120, 128, padding_idx=0)\n",
       "  (pos_embedding): Embedding(100, 128)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=4120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 定义 SASRec 模型（简化版 Transformer 序列推荐）\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len=100, embed_dim=128, num_heads=4,\n",
    "                 num_layers=4, ff_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T]\n",
    "        B, T = x.size()\n",
    "        positions = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "\n",
    "        seq_emb = self.embedding(x) + self.pos_embedding(positions)  # [B, T, E]\n",
    "\n",
    "        # padding mask: True 表示需要 mask\n",
    "        pad_mask = (x == PAD_ID)  # [B, T]\n",
    "\n",
    "        # subsequent mask: 保证只能看到当前位置及之前的 token\n",
    "        subsequent_mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "        out = self.encoder(\n",
    "            seq_emb,\n",
    "            mask=subsequent_mask,\n",
    "            src_key_padding_mask=pad_mask,\n",
    "        )  # [B, T, E]\n",
    "\n",
    "        logits = self.fc(out)  # [B, T, V]\n",
    "        return logits\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SASRec(vocab_size=vocab_size, max_len=100, embed_dim=128).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cebd863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss = 4.8054 | val_loss = 5.3426\n",
      "Epoch 2 | train_loss = 4.7795 | val_loss = 5.3451\n",
      "Epoch 3 | train_loss = 4.7546 | val_loss = 5.3515\n",
      "Epoch 4 | train_loss = 4.7317 | val_loss = 5.3559\n",
      "Epoch 5 | train_loss = 4.7083 | val_loss = 5.3563\n",
      "Epoch 6 | train_loss = 4.6853 | val_loss = 5.3606\n",
      "Epoch 7 | train_loss = 4.6642 | val_loss = 5.3671\n",
      "Epoch 8 | train_loss = 4.6414 | val_loss = 5.3716\n",
      "Epoch 9 | train_loss = 4.6201 | val_loss = 5.3835\n",
      "Epoch 10 | train_loss = 4.5984 | val_loss = 5.3814\n",
      "Epoch 11 | train_loss = 4.5798 | val_loss = 5.3936\n",
      "Epoch 12 | train_loss = 4.5595 | val_loss = 5.3967\n",
      "Epoch 13 | train_loss = 4.5429 | val_loss = 5.4052\n",
      "Epoch 14 | train_loss = 4.5211 | val_loss = 5.4157\n",
      "Epoch 15 | train_loss = 4.5035 | val_loss = 5.4204\n",
      "Epoch 16 | train_loss = 4.4895 | val_loss = 5.4327\n",
      "Epoch 17 | train_loss = 4.4705 | val_loss = 5.4360\n",
      "Epoch 18 | train_loss = 4.4559 | val_loss = 5.4476\n",
      "Epoch 19 | train_loss = 4.4376 | val_loss = 5.4539\n",
      "Epoch 20 | train_loss = 4.4201 | val_loss = 5.4591\n",
      "Epoch 21 | train_loss = 4.4063 | val_loss = 5.4715\n",
      "Epoch 22 | train_loss = 4.3919 | val_loss = 5.4775\n",
      "Epoch 23 | train_loss = 4.3776 | val_loss = 5.4837\n",
      "Epoch 24 | train_loss = 4.3640 | val_loss = 5.4903\n",
      "Epoch 25 | train_loss = 4.3498 | val_loss = 5.5031\n",
      "Epoch 26 | train_loss = 4.3361 | val_loss = 5.5092\n",
      "Epoch 27 | train_loss = 4.3254 | val_loss = 5.5160\n",
      "Epoch 28 | train_loss = 4.3141 | val_loss = 5.5262\n",
      "Epoch 29 | train_loss = 4.2999 | val_loss = 5.5337\n",
      "Epoch 30 | train_loss = 4.2894 | val_loss = 5.5387\n",
      "Early stopping at epoch 30 (no val improvement for 29 epochs).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finished'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 训练 SASRec（next item 重构），并在验证集上监控过拟合 + Early Stopping\n",
    "\n",
    "import copy\n",
    "\n",
    "num_epochs = 200  # 最大轮数，可根据资源调整\n",
    "patience = 29     # 连续多少个 epoch val_loss 不下降则早停\n",
    "best_val_loss = float('inf')\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------- 训练 ----------\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, targets, lengths = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)   # [B, T, V]\n",
    "\n",
    "        B, T, V = logits.shape\n",
    "        loss = criterion(logits.view(B * T, V), targets.view(B * T))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        total_steps += B\n",
    "\n",
    "    train_avg_loss = total_loss / max(total_steps, 1)\n",
    "\n",
    "    # ---------- 验证 ----------\n",
    "    model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    val_total_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, targets, lengths = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            B, T, V = logits.shape\n",
    "            loss = criterion(logits.view(B * T, V), targets.view(B * T))\n",
    "\n",
    "            val_total_loss += loss.item() * B\n",
    "            val_total_steps += B\n",
    "\n",
    "    val_avg_loss = val_total_loss / max(val_total_steps, 1)\n",
    "\n",
    "    print(f\"Epoch {epoch} | train_loss = {train_avg_loss:.4f} | val_loss = {val_avg_loss:.4f}\")\n",
    "\n",
    "    # ---------- Early Stopping & 保存最优模型 ----------\n",
    "    if val_avg_loss < best_val_loss:\n",
    "        best_val_loss = val_avg_loss\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        torch.save({'model_state_dict': best_state, 'item2id': item2id}, 'sasrec_model_best.pt')\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no val improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "# 训练结束后，加载最佳验证损失对应的权重\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6e0601b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 5.432594548070851)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 利用重构误差进行无监督异常检测（账户级），在测试集上评估\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "card_step_losses = {}\n",
    "step_losses_all = []\n",
    "step_recalls_all = []\n",
    "step_ndcgs_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq, card_id in zip(test_seqs, test_card_ids):\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        # 截断到与训练相同的 max_len\n",
    "        if len(seq) > 100:\n",
    "            seq_use = seq[-100:]\n",
    "        else:\n",
    "            seq_use = seq\n",
    "\n",
    "        inp = torch.tensor(seq_use[:-1], dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "        tgt = torch.tensor(seq_use[1:], dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "\n",
    "        logits = model(inp)  # [1, T, V]\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)  # [1, T, V]\n",
    "\n",
    "        T = tgt.size(1)\n",
    "        for t in range(T):\n",
    "            true_id = tgt[0, t].item()\n",
    "            if true_id == PAD_ID:\n",
    "                continue\n",
    "\n",
    "            lp = log_probs[0, t, true_id].item()\n",
    "            loss_t = -lp\n",
    "            step_losses_all.append(loss_t)\n",
    "\n",
    "            # 排名：按概率排序，计算 Recall@10 和 NDCG@10\n",
    "            probs_t = log_probs[0, t].exp().cpu().numpy()  # 转回概率\n",
    "            ranked_ids = np.argsort(-probs_t)\n",
    "\n",
    "            rank = np.where(ranked_ids == true_id)[0]\n",
    "            if len(rank) > 0:\n",
    "                rank = int(rank[0]) + 1  # 从 1 开始\n",
    "            else:\n",
    "                rank = None\n",
    "\n",
    "            if rank is not None and rank <= 10:\n",
    "                step_recalls_all.append(1.0)\n",
    "                step_ndcgs_all.append(1.0 / math.log2(rank + 1))\n",
    "            else:\n",
    "                step_recalls_all.append(0.0)\n",
    "                step_ndcgs_all.append(0.0)\n",
    "\n",
    "            card_step_losses.setdefault(card_id, []).append(loss_t)\n",
    "\n",
    "# 账户级异常分数：平均 loss（越大越异常）\n",
    "card_ids_scored = []\n",
    "card_scores = []\n",
    "card_steps = []\n",
    "\n",
    "for cid, losses in card_step_losses.items():\n",
    "    card_ids_scored.append(cid)\n",
    "    card_scores.append(float(np.mean(losses)))\n",
    "    card_steps.append(len(losses))\n",
    "\n",
    "len(card_ids_scored), np.mean(card_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2a62260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.3961035701737403,\n",
       " 'PR_AUC': 0.18319172815778018,\n",
       " 'Precision': 0.1404494382022472,\n",
       " 'Recall': 0.12315270935960591,\n",
       " 'F1': 0.13123359580052493}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. 账户级结果整理与评估（AUC、PR-AUC、Precision、Recall、F1）\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'card_id': card_ids_scored,\n",
    "    'sasrec_score': card_scores,   # 平均重构 loss，越大越异常\n",
    "    'num_steps': card_steps,\n",
    "})\n",
    "\n",
    "# 简单策略：使用 99 分位数作为异常阈值（可根据需要调整）\n",
    "threshold = result_df['sasrec_score'].quantile(0.8)\n",
    "result_df['sasrec_label'] = (result_df['sasrec_score'] >= threshold).astype(int)  # 1=异常, 0=正常\n",
    "\n",
    "# 账户级真实标签：按 card_id 聚合明细标签，这里用 max 规则（账户内只要有一条是 1，就认为账户为 1）\n",
    "card_label = (\n",
    "    df.groupby('card_id')['label']\n",
    "    .max()\n",
    "    .reindex(result_df['card_id'])  # 按 result_df 对齐\n",
    ")\n",
    "\n",
    "# 转成 numpy 数组\n",
    "y_true = card_label.values.astype(int)\n",
    "\n",
    "# sasrec_score 本身就是“越大越异常”，可以直接作为异常分数\n",
    "anomaly_prob = result_df['sasrec_score'].values\n",
    "\n",
    "# AUC-ROC\n",
    "auc = roc_auc_score(y_true, anomaly_prob)\n",
    "\n",
    "# PR-AUC（Average Precision）\n",
    "pr_auc = average_precision_score(y_true, anomaly_prob)\n",
    "\n",
    "# 二值预测：使用 sasrec_label（1=异常，0=正常）\n",
    "y_pred = result_df['sasrec_label'].values\n",
    "\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "metrics = {\n",
    "    'AUC': auc,\n",
    "    'PR_AUC': pr_auc,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1': f1,\n",
    "}\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d24f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Avg_Loss': 5.3694311141630715,\n",
       " 'Recall@10': 0.2855103971322834,\n",
       " 'NDCG@10': 0.16067002431333097}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. 序列级指标：Avg_Loss、Recall@10、NDCG@10\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "avg_loss = float(np.mean(step_losses_all)) if step_losses_all else float('nan')\n",
    "recall_at_10 = float(np.mean(step_recalls_all)) if step_recalls_all else float('nan')\n",
    "ndcg_at_10 = float(np.mean(step_ndcgs_all)) if step_ndcgs_all else float('nan')\n",
    "\n",
    "seq_metrics = {\n",
    "    'Avg_Loss': avg_loss,\n",
    "    'Recall@10': recall_at_10,\n",
    "    'NDCG@10': ndcg_at_10,\n",
    "}\n",
    "\n",
    "seq_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57a500-8c8d-4db5-ac9e-9e41172043be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad_env",
   "language": "python",
   "name": "gad_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
