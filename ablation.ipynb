{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54632cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# SeqRecon-AD 消融实验：Full（finetune embedding + self-clean）vs NoClean、MeanAgg、RandEmb、FreezeEmb\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score, precision_score, recall_score\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767b7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=7133, val=892, test=892, num_items=4119\n"
     ]
    }
   ],
   "source": [
    "# 1. 数据加载（与 SeqRecon-AD 一致）\n",
    "\n",
    "card_item = pd.read_csv('card_item.csv')\n",
    "card_feats = pd.read_csv('card_feats.csv', usecols=['label','card_id','name','身份证号','age'])\n",
    "dataset = pd.concat([card_item, card_feats], axis=1)\n",
    "\n",
    "import ast\n",
    "if isinstance(dataset['明细项目名称'].iloc[0], str):\n",
    "    dataset['明细项目名称'] = dataset['明细项目名称'].apply(ast.literal_eval)\n",
    "\n",
    "with open('item2id.json', 'r', encoding='utf-8') as f:\n",
    "    item2id = json.load(f)\n",
    "id2item = {v: k for k, v in item2id.items()}\n",
    "num_items = len(item2id)\n",
    "\n",
    "def map_items_to_ids(items, item2id):\n",
    "    return [item2id[item] for item in items if item in item2id]\n",
    "dataset['明细项目ID'] = dataset['明细项目名称'].apply(lambda x: map_items_to_ids(x, item2id))\n",
    "\n",
    "train_df, temp_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "print(f\"train={len(train_df)}, val={len(val_df)}, test={len(test_df)}, num_items={num_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed35421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerAnomalyDetectorAblation ready.\n"
     ]
    }
   ],
   "source": [
    "# 2. 位置感知 Transformer + 消融参数（MeanAgg / RandEmb / FreezeEmb）；异常分数不做账户内 z-score\n",
    "\n",
    "class RelativePositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_heads, max_len=512):\n",
    "        super().__init__()\n",
    "        self.rel_pos_table = nn.Parameter(torch.randn(2 * max_len - 1, num_heads))\n",
    "        self.max_len = max_len\n",
    "    def forward(self, q_len, k_len):\n",
    "        range_q = torch.arange(q_len)[:, None]\n",
    "        range_k = torch.arange(k_len)[None, :]\n",
    "        distance_mat = range_k - range_q\n",
    "        distance_mat = distance_mat.clamp(-self.max_len + 1, self.max_len - 1) + self.max_len - 1\n",
    "        rel_bias = self.rel_pos_table[distance_mat].permute(2, 0, 1)\n",
    "        return rel_bias\n",
    "\n",
    "class RelativeMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.num_heads, self.dropout = embed_dim, num_heads, dropout\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, pos_bias=None):\n",
    "        B, L, D = query.shape\n",
    "        H, d = self.num_heads, D // self.num_heads\n",
    "        q = self.q_proj(query).view(B, L, H, d).transpose(1, 2)\n",
    "        k = self.k_proj(key).view(B, L, H, d).transpose(1, 2)\n",
    "        v = self.v_proj(value).view(B, L, H, d).transpose(1, 2)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d)\n",
    "        if pos_bias is not None: scores = scores + pos_bias.unsqueeze(0)\n",
    "        if attn_mask is not None: scores = scores + attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "        if key_padding_mask is not None: scores = scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n",
    "        attn_weights = F.dropout(torch.softmax(scores, dim=-1), p=self.dropout, training=self.training)\n",
    "        out = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(B, L, D)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.self_attn = RelativeMultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None, pos_bias=None):\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask, pos_bias=pos_bias)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src = src + self.dropout2(self.linear2(F.relu(self.linear1(src))))\n",
    "        return self.norm2(src)\n",
    "\n",
    "class TransformerAnomalyDetectorAblation(nn.Module):\n",
    "    \"\"\"支持消融：use_topk_agg(MeanAgg), freeze_embedding(FreezeEmb)；use_score_norm 已统一为 False。\"\"\"\n",
    "    def __init__(self, embedding_matrix, d_model=512, nhead=4, num_layers=6, dropout=0.2, pad_idx=0,\n",
    "                 use_score_norm=False, use_topk_agg=True, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        self.use_score_norm = use_score_norm\n",
    "        self.use_topk_agg = use_topk_agg\n",
    "        num_items, embedding_dim = embedding_matrix.size()\n",
    "        emb = (embedding_matrix - embedding_matrix.mean()) / (embedding_matrix.std() + 1e-8)\n",
    "        self.embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(emb.clone().detach())\n",
    "        self.embedding.weight.requires_grad = not freeze_embedding\n",
    "        self.embed_proj = nn.Linear(embedding_dim, d_model)\n",
    "        self.pos_encoder = RelativePositionalEncoding(num_heads=nhead, max_len=512)\n",
    "        self.layers = nn.ModuleList([CustomTransformerEncoderLayer(d_model, nhead, dropout) for _ in range(num_layers)])\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.predictor = nn.Linear(d_model, num_items)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embed_proj.bias.data.zero_()\n",
    "        self.embed_proj.weight.data.uniform_(-initrange, initrange)\n",
    "        nn.init.xavier_uniform_(self.predictor.weight)\n",
    "        self.predictor.bias.data.zero_()\n",
    "\n",
    "    def generate_mask(self, seq_len, device):\n",
    "        m = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "        m = m.float().masked_fill(m == 0, float('-inf')).masked_fill(m == 1, 0.0)\n",
    "        return m.to(device)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        B, L = src.size()\n",
    "        src_emb = self.embedding(src)\n",
    "        src_emb = self.embed_proj(src_emb) * math.sqrt(self.d_model)\n",
    "        src_emb = F.layer_norm(src_emb, src_emb.shape[-1:])\n",
    "        pos_bias = self.pos_encoder(L, L)\n",
    "        pad_mask = (src == self.pad_idx)\n",
    "        out = src_emb\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, src_mask=src_mask, src_key_padding_mask=pad_mask, pos_bias=pos_bias)\n",
    "        return self.predictor(self.final_norm(out))\n",
    "\n",
    "    def compute_loss(self, src, tgt, mask=None):\n",
    "        seq_len = src.size(1)\n",
    "        causal_mask = self.generate_mask(seq_len, src.device)\n",
    "        predictions = self.forward(src, src_mask=causal_mask)[:, :-1, :].contiguous()\n",
    "        tgt = tgt[:, 1:].contiguous()\n",
    "        if mask is not None:\n",
    "            mask = mask[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(predictions.view(-1, predictions.size(-1)), tgt.view(-1), reduction='none')\n",
    "            loss = loss[mask.view(-1) == 1].mean()\n",
    "        else:\n",
    "            loss = F.cross_entropy(predictions.view(-1, predictions.size(-1)), tgt.view(-1), ignore_index=self.pad_idx)\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            return torch.tensor(0.0, requires_grad=True).to(loss.device)\n",
    "        return loss\n",
    "\n",
    "    def compute_anomaly_score(self, sequences, mask=None, topk_ratio=0.2, return_token_level=False):\n",
    "        with torch.no_grad():\n",
    "            seq_len = sequences.size(1)\n",
    "            causal_mask = self.generate_mask(seq_len, sequences.device)\n",
    "            predictions = self.forward(sequences, src_mask=causal_mask)[:, :-1, :].contiguous()\n",
    "            targets = sequences[:, 1:].contiguous()\n",
    "            per_position_loss = F.cross_entropy(\n",
    "                predictions.view(-1, predictions.size(-1)), targets.view(-1), reduction='none'\n",
    "            ).view_as(targets)\n",
    "            if mask is not None:\n",
    "                mask_cut = mask[:, 1:].contiguous()\n",
    "                per_position_loss = per_position_loss * mask_cut\n",
    "            if self.use_score_norm:\n",
    "                mean = per_position_loss.mean(dim=1, keepdim=True)\n",
    "                std = per_position_loss.std(dim=1, keepdim=True) + 1e-8\n",
    "                normalized_loss = (per_position_loss - mean) / std\n",
    "            else:\n",
    "                normalized_loss = per_position_loss\n",
    "            if self.use_topk_agg:\n",
    "                k = max(1, int(topk_ratio * (seq_len - 1)))\n",
    "                topk_values, _ = torch.topk(normalized_loss, k=k, dim=1)\n",
    "                scores = topk_values.mean(dim=1)\n",
    "            else:\n",
    "                if mask is not None:\n",
    "                    scores = (normalized_loss * mask_cut).sum(dim=1) / (mask_cut.sum(dim=1) + 1e-8)\n",
    "                else:\n",
    "                    scores = normalized_loss.mean(dim=1)\n",
    "            if return_token_level:\n",
    "                return scores.cpu().numpy(), normalized_loss.cpu().numpy()\n",
    "            return scores.cpu().numpy()\n",
    "\n",
    "print('TransformerAnomalyDetectorAblation ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a59f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset / DataLoader\n",
    "\n",
    "class PrescriptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_length=517, pad_idx=0):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.max_length = max_length\n",
    "        self.pad_idx = pad_idx\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data.iloc[idx]['明细项目ID']\n",
    "        if len(sequence) > self.max_length:\n",
    "            sequence = sequence[:self.max_length]\n",
    "            original_len = self.max_length\n",
    "        else:\n",
    "            original_len = len(sequence)\n",
    "            sequence = sequence + [self.pad_idx] * (self.max_length - len(sequence))\n",
    "        mask = [1] * original_len + [0] * (self.max_length - original_len)\n",
    "        return {\n",
    "            'input_seq': torch.tensor(sequence, dtype=torch.long),\n",
    "            'target_seq': torch.tensor(sequence, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.float),\n",
    "            'label': torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    return {\n",
    "        'input_seq': torch.stack([b['input_seq'] for b in batch]),\n",
    "        'target_seq': torch.stack([b['target_seq'] for b in batch]),\n",
    "        'mask': torch.stack([b['mask'] for b in batch]),\n",
    "        'label': torch.stack([b['label'] for b in batch]),\n",
    "    }\n",
    "\n",
    "train_dataset = PrescriptionDataset(train_df)\n",
    "val_dataset = PrescriptionDataset(val_df)\n",
    "test_dataset = PrescriptionDataset(test_df)\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a827d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 评估函数：Recall@10 / NDCG@10 & 异常检测（AUC, PR-AUC, F1）\n",
    "\n",
    "def evaluate_retrieval(model, data_loader, device, k=10):\n",
    "    model.eval()\n",
    "    total_recall, total_ndcg, total_cnt = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_seq = batch['input_seq'].to(device)\n",
    "            target_seq = batch['target_seq'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            B, L = input_seq.size()\n",
    "            causal_mask = model.generate_mask(L, device)\n",
    "            logits = model(input_seq, src_mask=causal_mask)\n",
    "            last_logits = logits[:, -1, :].clone()\n",
    "            last_logits[:, 0] = -float('inf')\n",
    "            _, topk_indices = torch.topk(last_logits, k=k, dim=-1)\n",
    "            lengths = mask.sum(dim=1).long()\n",
    "            next_indices = (lengths - 1).clamp(min=0)\n",
    "            next_item = target_seq.gather(1, next_indices.view(-1, 1)).squeeze(1)\n",
    "            gt = next_item.cpu().numpy()\n",
    "            pred = topk_indices.cpu().numpy()\n",
    "            for g, p in zip(gt, pred):\n",
    "                if np.any(p == g):\n",
    "                    total_recall += 1.0\n",
    "                    rank = np.where(p == g)[0][0] + 1\n",
    "                    total_ndcg += 1.0 / math.log2(rank + 1)\n",
    "                total_cnt += 1\n",
    "    return total_recall / total_cnt, total_ndcg / total_cnt\n",
    "\n",
    "def evaluate_model_top(model, loader, device, top_percent=0.2):\n",
    "    all_scores, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Eval'):\n",
    "            scores = model.compute_anomaly_score(batch['input_seq'].to(device), batch['mask'].to(device), topk_ratio=0.2)\n",
    "            scores = np.nan_to_num(scores, nan=0.0)\n",
    "            all_scores.extend(scores)\n",
    "            all_labels.extend(batch['label'].numpy())\n",
    "    all_scores = np.asarray(all_scores)\n",
    "    all_labels = np.asarray(all_labels)\n",
    "    cutoff = np.percentile(all_scores, 100 * (1 - top_percent))\n",
    "    pred_labels = (all_scores >= cutoff).astype(int)\n",
    "    prec, rec, _ = precision_recall_curve(all_labels, all_scores)\n",
    "    return {\n",
    "        'auc': roc_auc_score(all_labels, all_scores),\n",
    "        'pr_auc': auc(rec, prec),\n",
    "        'f1': f1_score(all_labels, pred_labels, zero_division=0),\n",
    "        'precision': precision_score(all_labels, pred_labels, zero_division=0),\n",
    "        'recall': recall_score(all_labels, pred_labels, zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e186ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 训练循环：带自清洗 (Full/MeanAgg/RandEmb/FreezeEmb) 与 无自清洗 (NoClean)\n",
    "\n",
    "import copy\n",
    "\n",
    "def self_cleaning_training_loopv3(model, original_dataset, original_val_dataset, device,\n",
    "                                  max_epochs=150, clean_start_epoch=20, clean_ratio=0.2,\n",
    "                                  patience=5, batch_size=128, k=10, eval_fn=evaluate_retrieval,\n",
    "                                  save_path='ablation.pt'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
    "    best_recall, best_ndcg, epochs_no_improve = 0.0, 0.0, 0\n",
    "    clean_mode, clean_count = False, 0\n",
    "    current_indices = list(range(len(original_dataset)))\n",
    "    current_dataset = Subset(original_dataset, current_indices)\n",
    "    train_loader = DataLoader(current_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(original_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    best_result = None\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_batches = 0.0, 0\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{max_epochs}'):\n",
    "            input_seq = batch['input_seq'].to(device)\n",
    "            target_seq = batch['target_seq'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(input_seq, target_seq, mask)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / total_batches\n",
    "        model.eval()\n",
    "        recall_k, ndcg_k = eval_fn(model, val_loader, device, k=k)\n",
    "        print(f'Epoch {epoch+1} Loss: {avg_loss:.4f} Val R@{k}: {recall_k:.4f} NDCG@{k}: {ndcg_k:.4f}')\n",
    "        if recall_k > best_recall:\n",
    "            best_recall, best_ndcg = recall_k, ndcg_k\n",
    "            best_result = {'epoch': epoch+1, 'avg_loss': avg_loss, 'recall': recall_k, 'ndcg': ndcg_k}\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if (epoch + 1 >= clean_start_epoch or clean_mode) and epochs_no_improve >= patience:\n",
    "            print(f'Self-cleaning round {clean_count+1}...')\n",
    "            model.eval()\n",
    "            all_scores = []\n",
    "            for batch in DataLoader(Subset(original_dataset, current_indices), batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn):\n",
    "                all_scores.append(model.compute_anomaly_score(batch['input_seq'].to(device), batch['mask'].to(device)))\n",
    "            all_scores = np.concatenate(all_scores)\n",
    "            valid = (~np.isnan(all_scores)) & (all_scores > 0)\n",
    "            if valid.sum() > 0:\n",
    "                thresh = np.quantile(all_scores[valid], 1 - clean_ratio)\n",
    "                keep = np.where(all_scores < thresh)[0]\n",
    "                current_indices = [current_indices[i] for i in keep]\n",
    "                current_dataset = Subset(original_dataset, current_indices)\n",
    "                train_loader = DataLoader(current_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "                print(f'Retained {len(current_indices)} / {len(original_dataset)} samples.')\n",
    "            clean_mode, clean_count, epochs_no_improve = True, clean_count + 1, 0\n",
    "    model.best_result = best_result\n",
    "    return model\n",
    "\n",
    "def train_no_cleaning(model, train_loader, val_loader, device, max_epochs=150, patience=5, batch_size=128, k=10,\n",
    "                      eval_fn=evaluate_retrieval, save_path='ablation.pt'):\n",
    "    \"\"\"NoClean：无自清洗，仅按 Val Recall@k 早停。\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
    "    best_recall, best_ndcg, best_result, epochs_no_improve = 0.0, 0.0, None, 0\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_batches = 0.0, 0\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{max_epochs}'):\n",
    "            input_seq = batch['input_seq'].to(device)\n",
    "            target_seq = batch['target_seq'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(input_seq, target_seq, mask)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / total_batches\n",
    "        model.eval()\n",
    "        recall_k, ndcg_k = eval_fn(model, val_loader, device, k=k)\n",
    "        print(f'Epoch {epoch+1} Loss: {avg_loss:.4f} Val R@{k}: {recall_k:.4f} NDCG@{k}: {ndcg_k:.4f}')\n",
    "        if recall_k > best_recall:\n",
    "            best_recall, best_ndcg = recall_k, ndcg_k\n",
    "            best_result = {'epoch': epoch+1, 'avg_loss': avg_loss, 'recall': recall_k, 'ndcg': ndcg_k}\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stop at epoch {epoch+1}.')\n",
    "                break\n",
    "    model.best_result = best_result\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60fafbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_464792/4077359593.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_emb.load_state_dict(torch.load('item_embedding.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs: ['Full', 'NoClean', 'MeanAgg', 'RandEmb', 'FreezeEmb']\n"
     ]
    }
   ],
   "source": [
    "# 6. 加载 PULSE 嵌入与消融配置\n",
    "\n",
    "embedding_dim = 4096\n",
    "pretrained_emb = nn.Embedding(num_items, embedding_dim)\n",
    "pretrained_emb.load_state_dict(torch.load('item_embedding.pt'))\n",
    "with torch.no_grad():\n",
    "    embedding_matrix_pulse = pretrained_emb.weight.clone().detach()\n",
    "\n",
    "# RandEmb 用随机初始化（同 shape）\n",
    "torch.manual_seed(42)\n",
    "embedding_matrix_rand = torch.randn(num_items, embedding_dim) * 0.02\n",
    "\n",
    "# Full = finetune embedding + self-clean；对比：NoClean、MeanAgg、RandEmb、FreezeEmb\n",
    "ABLATION_CONFIGS = [\n",
    "    {'name': 'Full',     'use_cleaning': True,  'use_topk_agg': True,  'embedding': embedding_matrix_pulse, 'freeze_embedding': False},\n",
    "    {'name': 'NoClean',  'use_cleaning': False, 'use_topk_agg': True,  'embedding': embedding_matrix_pulse, 'freeze_embedding': False},\n",
    "    {'name': 'MeanAgg',  'use_cleaning': True,  'use_topk_agg': False, 'embedding': embedding_matrix_pulse, 'freeze_embedding': False},\n",
    "    {'name': 'RandEmb',  'use_cleaning': True,  'use_topk_agg': True,  'embedding': embedding_matrix_rand,  'freeze_embedding': True},\n",
    "    {'name': 'FreezeEmb','use_cleaning': True,  'use_topk_agg': True,  'embedding': embedding_matrix_pulse, 'freeze_embedding': True},\n",
    "]\n",
    "print('Configs:', [c['name'] for c in ABLATION_CONFIGS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ba765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n========== Full ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/150: 100%|██████████| 56/56 [00:42<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.7690 Val R@10: 0.1166 NDCG@10: 0.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/150: 100%|██████████| 56/56 [00:42<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.8343 Val R@10: 0.1603 NDCG@10: 0.0939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/150: 100%|██████████| 56/56 [00:42<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 5.6393 Val R@10: 0.1648 NDCG@10: 0.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/150: 100%|██████████| 56/56 [00:42<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 5.5462 Val R@10: 0.1827 NDCG@10: 0.1077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/150: 100%|██████████| 56/56 [00:40<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 5.4647 Val R@10: 0.1973 NDCG@10: 0.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/150: 100%|██████████| 56/56 [00:41<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 5.3940 Val R@10: 0.2040 NDCG@10: 0.1261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/150: 100%|██████████| 56/56 [00:41<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 5.3263 Val R@10: 0.2108 NDCG@10: 0.1323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/150: 100%|██████████| 56/56 [00:40<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 5.2658 Val R@10: 0.2242 NDCG@10: 0.1378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/150: 100%|██████████| 56/56 [00:40<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 5.2091 Val R@10: 0.2466 NDCG@10: 0.1472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/150: 100%|██████████| 56/56 [00:40<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 5.1604 Val R@10: 0.2466 NDCG@10: 0.1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/150: 100%|██████████| 56/56 [00:42<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 5.1083 Val R@10: 0.2478 NDCG@10: 0.1505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/150: 100%|██████████| 56/56 [00:42<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 5.0664 Val R@10: 0.2668 NDCG@10: 0.1566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/150: 100%|██████████| 56/56 [00:42<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 5.0257 Val R@10: 0.2668 NDCG@10: 0.1593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/150: 100%|██████████| 56/56 [00:42<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 4.9869 Val R@10: 0.2836 NDCG@10: 0.1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/150: 100%|██████████| 56/56 [00:42<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 4.9539 Val R@10: 0.2892 NDCG@10: 0.1696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/150: 100%|██████████| 56/56 [00:42<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 4.9179 Val R@10: 0.2848 NDCG@10: 0.1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/150: 100%|██████████| 56/56 [00:42<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 4.8897 Val R@10: 0.2993 NDCG@10: 0.1761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/150: 100%|██████████| 56/56 [00:42<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 4.8599 Val R@10: 0.3027 NDCG@10: 0.1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/150: 100%|██████████| 56/56 [00:40<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 4.8250 Val R@10: 0.2971 NDCG@10: 0.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/150: 100%|██████████| 56/56 [00:40<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 4.7990 Val R@10: 0.3094 NDCG@10: 0.1805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/150: 100%|██████████| 56/56 [00:40<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Loss: 4.7770 Val R@10: 0.3004 NDCG@10: 0.1773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/150: 100%|██████████| 56/56 [00:40<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Loss: 4.7560 Val R@10: 0.3195 NDCG@10: 0.1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/150: 100%|██████████| 56/56 [00:42<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Loss: 4.7303 Val R@10: 0.3027 NDCG@10: 0.1834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/150: 100%|██████████| 56/56 [00:42<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Loss: 4.7060 Val R@10: 0.3195 NDCG@10: 0.1863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/150: 100%|██████████| 56/56 [00:42<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Loss: 4.6861 Val R@10: 0.3229 NDCG@10: 0.1902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/150: 100%|██████████| 56/56 [00:42<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Loss: 4.6647 Val R@10: 0.3184 NDCG@10: 0.1898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/150: 100%|██████████| 56/56 [00:41<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Loss: 4.6445 Val R@10: 0.3128 NDCG@10: 0.1860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/150: 100%|██████████| 56/56 [00:41<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Loss: 4.6253 Val R@10: 0.3229 NDCG@10: 0.1902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/150: 100%|██████████| 56/56 [00:41<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Loss: 4.6059 Val R@10: 0.3083 NDCG@10: 0.1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/150: 100%|██████████| 56/56 [00:40<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Loss: 4.5916 Val R@10: 0.3195 NDCG@10: 0.1873\n",
      "Self-cleaning round 1...\n",
      "Retained 6420 / 7133 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/150: 100%|██████████| 51/51 [00:37<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Loss: 4.5588 Val R@10: 0.3173 NDCG@10: 0.1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/150: 100%|██████████| 51/51 [00:37<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Loss: 4.5347 Val R@10: 0.3184 NDCG@10: 0.1877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/150: 100%|██████████| 51/51 [00:37<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Loss: 4.4992 Val R@10: 0.3240 NDCG@10: 0.1876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/150: 100%|██████████| 51/51 [00:38<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Loss: 4.4777 Val R@10: 0.3274 NDCG@10: 0.1888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/150:  41%|████      | 21/51 [00:16<00:23,  1.30it/s]"
     ]
    }
   ],
   "source": [
    "# 7. 逐组训练并评测，汇总结果表\n",
    "\n",
    "max_epochs = 150\n",
    "clean_start_epoch = 20\n",
    "clean_ratio = 0.1\n",
    "patience = 5\n",
    "k = 10\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for cfg in ABLATION_CONFIGS:\n",
    "    name = cfg['name']\n",
    "    save_path = f'ablation_{name}.pt'\n",
    "    print(f'\\\\n========== {name} ==========')\n",
    "    model = TransformerAnomalyDetectorAblation(\n",
    "        cfg['embedding'],\n",
    "        d_model=512, nhead=2, num_layers=8, dropout=0.1, pad_idx=0,\n",
    "        use_score_norm=False,\n",
    "        use_topk_agg=cfg['use_topk_agg'],\n",
    "        freeze_embedding=cfg['freeze_embedding'],\n",
    "    ).to(device)\n",
    "    if cfg['use_cleaning']:\n",
    "        model = self_cleaning_training_loopv3(\n",
    "            model, train_dataset, val_dataset, device,\n",
    "            max_epochs=max_epochs, clean_start_epoch=clean_start_epoch, clean_ratio=clean_ratio,\n",
    "            patience=patience, batch_size=batch_size, k=k, save_path=save_path,\n",
    "        )\n",
    "    else:\n",
    "        model = train_no_cleaning(\n",
    "            model, train_loader, val_loader, device,\n",
    "            max_epochs=max_epochs, patience=patience, batch_size=batch_size, k=k, save_path=save_path,\n",
    "        )\n",
    "    # 加载最佳权重并在测试集上评测\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    test_loss_sum, test_n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch['input_seq'].to(device)\n",
    "            y = batch['target_seq'].to(device)\n",
    "            m = batch['mask'].to(device)\n",
    "            test_loss_sum += model.compute_loss(x, y, m).item() * x.size(0)\n",
    "            test_n += x.size(0)\n",
    "    avg_loss = test_loss_sum / max(test_n, 1)\n",
    "    recall10, ndcg10 = evaluate_retrieval(model, test_loader, device, k=k)\n",
    "    ad_metrics = evaluate_model_top(model, test_loader, device, top_percent=0.2)\n",
    "    results_list.append({\n",
    "        'Variant': name,\n",
    "        'Avg_Loss': round(avg_loss, 4),\n",
    "        'Recall@10': round(recall10, 4),\n",
    "        'NDCG@10': round(ndcg10, 4),\n",
    "        'AUC': round(ad_metrics['auc'], 4),\n",
    "        'PR-AUC': round(ad_metrics['pr_auc'], 4),\n",
    "        'Precision': round(ad_metrics['precision'], 4),\n",
    "        'Recall': round(ad_metrics['recall'], 4),\n",
    "        'F1': round(ad_metrics['f1'], 4),\n",
    "    })\n",
    "    # 每组跑完后释放显存，避免多组累积 OOM\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# 最终评测指标：Avg_Loss, Recall@10, NDCG@10, AUC, PR-AUC, Precision, Recall, F1\n",
    "ablation_df = pd.DataFrame(results_list)\n",
    "cols = ['Variant', 'Avg_Loss', 'Recall@10', 'NDCG@10', 'AUC', 'PR-AUC', 'Precision', 'Recall', 'F1']\n",
    "ablation_df = ablation_df[cols]\n",
    "print('\\\\n========== 消融结果汇总（Full=finetune emb+self-clean）==========')\n",
    "display(ablation_df)\n",
    "ablation_df.to_csv('ablation_results.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd9d44-3e65-41a4-8a0a-ade6a1ffb71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad_env",
   "language": "python",
   "name": "gad_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
